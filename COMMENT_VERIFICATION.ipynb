{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TITLE_ = 'title'\n",
    "_COMMENT_ = 'comment'\n",
    "_TITLE_AND_COMMENT_ = 'title_comment'\n",
    "_VERIF_STAT_ = 'verification_status'\n",
    "_ID_ = 'id'\n",
    "\n",
    "_CONST_ = 24.8\n",
    "_PRECISION_ = (3/4)\n",
    "_PRECISION1_ = 24\n",
    "_PRECISION2_= 0.00803982472\n",
    "\n",
    "_TRAINING_SET_PATH_ = '/home/amirh/Documents/AIHOOSH/train.csv'\n",
    "_TEST_SET_PATH_ = '/home/amirh/Documents/AIHOOSH/test.csv'\n",
    "_STOP_WORDS_PATH_ = '/home/amirh/Documents/AIHOOSH/stopwords-fa.txt'\n",
    "_RESULT_PATH_ = '/home/amirh/Documents/AIHOOSH/ans.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneColumn_set(data_frame : pd.DataFrame):\n",
    "    data_frame[_TITLE_AND_COMMENT_] = data_frame[_TITLE_] + ' ' + data_frame[_COMMENT_]\n",
    "    data_frame = data_frame.drop(columns = [_COMMENT_, _TITLE_] ,axis = 1)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_use(training_setX : pd.DataFrame, training_setY : pd.DataFrame):\n",
    "    \n",
    "    # a function to calculate all words used in data_frame\n",
    "    using_words = dict() #first arg value , sec arg tuple\n",
    "    \n",
    "    number_count_valid = 0\n",
    "    number_count_notValid = 0\n",
    "    \n",
    "    for index, row in training_setX.iterrows():\n",
    "        words = str(row['title_comment']).split()\n",
    "        \n",
    "        if training_setY[index] == 0:#valid\n",
    "            number_count_valid += len(words)\n",
    "        else:\n",
    "            number_count_notValid += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in using_words.keys(): #new word stat if not in dict\n",
    "                if training_setY[index] == 0:    #if valid\n",
    "                    using_words.update({word : (1, 0)}) #update dict of valids to 1\n",
    "                elif training_setY[index] == 1:\n",
    "                    using_words.update({word : (0, 1)}) #\n",
    "            else:\n",
    "                old_tuple = using_words[word]\n",
    "                if training_setY[index] == 0:\n",
    "                    new_tuple = (old_tuple[0] + 1, old_tuple[1])\n",
    "                    using_words.update({word : new_tuple})\n",
    "                elif training_setY[index] == 1:\n",
    "                    new_tuple = (old_tuple[0], old_tuple[1] + 1)\n",
    "                    using_words.update({word : new_tuple})\n",
    "        \n",
    "    return using_words, number_count_valid, number_count_notValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stop_words(data_frame : pd.DataFrame):\n",
    "    stop_words = pd.read_csv(_STOP_WORDS_PATH_)\n",
    "    data_frame[_TITLE_AND_COMMENT_].apply(lambda x: [item for item in data_frame if item not in stop_words])\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_valid_words(words : dict, number_count_valid, number_count_notValid):\n",
    "    predictions = dict()\n",
    "    \n",
    "    number_uniuqe_words = len(words.keys())\n",
    "    \n",
    "    for key, value in words.items():\n",
    "        p_word_valid = (value[0] + _CONST_) / (number_count_valid + number_uniuqe_words)\n",
    "        p_word_notValid = (value[1] + _CONST_) / (number_count_notValid + number_uniuqe_words)\n",
    "        predictions.update({key : (p_word_valid, p_word_notValid)})\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_value(title_comment : str, predictions : dict, p_valid):\n",
    "    words = title_comment.split()\n",
    "    p_notValid = 1 - p_valid + _PRECISION_ \n",
    "    for word in words:\n",
    "        if word in predictions.keys():\n",
    "            p_valid *= _PRECISION1_*predictions[word][0]*_PRECISION1_\n",
    "            p_notValid *= _PRECISION1_*predictions[word][1]*_PRECISION1_\n",
    "        else:\n",
    "            p_valid *= _PRECISION2_\n",
    "            p_notValid *= _PRECISION2_\n",
    "    return (0 if p_valid >= p_notValid else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "\n",
    "# clear training set and test set\n",
    "training_set = pd.read_csv(_TRAINING_SET_PATH_)[[_TITLE_ , _COMMENT_ , _VERIF_STAT_]]\n",
    "test_set     = pd.read_csv(_TEST_SET_PATH_)[[_ID_ , _TITLE_ , _COMMENT_]]\n",
    "\n",
    "# # delete rows which contains nan values\n",
    "# training_set = training_set.dropna(axis = 'rows')\n",
    "# test_set = test_set.dropna(axis = 'rows')\n",
    "\n",
    "# split to training_setX and training_setY\n",
    "training_setX = training_set[[_TITLE_ , _COMMENT_]]\n",
    "training_setY = training_set[_VERIF_STAT_]\n",
    "\n",
    "test_setX = test_set[[_TITLE_, _COMMENT_]]\n",
    "\n",
    "# append column title to column comment\n",
    "training_setX = make_oneColumn_set(training_setX)\n",
    "test_setX    = make_oneColumn_set(test_setX)\n",
    "\n",
    "# delete punctuation from our training set\n",
    "training_setx = training_setX[_TITLE_AND_COMMENT_].str.replace('[^\\w\\s]','')\n",
    "test_setx = test_setX[_TITLE_AND_COMMENT_].str.replace('[^\\w\\s]','')\n",
    "\n",
    "training_setX[_TITLE_AND_COMMENT_] = delete_stop_words(training_setX)\n",
    "test_setX[_TITLE_AND_COMMENT_] = delete_stop_words(test_setX)\n",
    "\n",
    "p_valid = len(training_setY[training_setY == 0]) / len(training_setY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_words, number_cValid, number_cNotValid = all_words_use(training_setX, training_setY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = prediction_valid_words(using_words, number_cValid, number_cNotValid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = [_ID_ , _TEST_SET_PATH_])\n",
    "for index, row in test_setX.iterrows():\n",
    "    pred = predict_value(str(row[ _TITLE_AND_COMMENT_ ]) , predictions , p_valid)\n",
    "    results.loc[index] = [test_set[_ID_][index], pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(_RESULT_PATH_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
